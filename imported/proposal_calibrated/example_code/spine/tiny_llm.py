"""Tiny Transformer language model used by the validation experiment.

The manuscript is explicitly written for *LLM-driven* systems: a frozen Transformer
backbone produces text-context features (z_t), and a small LoRA adapter is the only
trainable component during continual updates.

To keep the proof-of-concept self-contained (no external model downloads), the
validation experiment uses a very small causal Transformer that is pretrained on a
synthetic token stream generated by the market simulator.

This module provides:
  - a minimal causal LM (next-token prediction)
  - a stable way to extract a per-sequence embedding used as z_t
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Optional, Tuple

import torch
from torch import nn


@dataclass(frozen=True)
class TinyLMConfig:
    """Configuration for the tiny causal Transformer LM."""

    vocab_size: int
    max_seq_len: int
    d_model: int = 32
    n_heads: int = 4
    n_layers: int = 2
    d_ff: int = 128
    dropout: float = 0.0


def _causal_mask(seq_len: int, device: torch.device) -> torch.Tensor:
    """Return an additive causal mask (L,L) with -inf above the diagonal."""

    m = torch.full((seq_len, seq_len), float("-inf"), device=device)
    return torch.triu(m, diagonal=1)


class TinyCausalTransformerLM(nn.Module):
    """A small decoder-style Transformer implemented via a causal encoder stack."""

    def __init__(self, cfg: TinyLMConfig):
        super().__init__()
        self.cfg = cfg

        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.d_model)
        self.pos_emb = nn.Embedding(cfg.max_seq_len, cfg.d_model)

        enc_layer = nn.TransformerEncoderLayer(
            d_model=cfg.d_model,
            nhead=cfg.n_heads,
            dim_feedforward=cfg.d_ff,
            dropout=cfg.dropout,
            batch_first=True,
            activation="gelu",
        )
        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=cfg.n_layers)
        self.lm_head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)

        # Small init helps stability for tiny models.
        nn.init.normal_(self.tok_emb.weight, mean=0.0, std=0.02)
        nn.init.normal_(self.pos_emb.weight, mean=0.0, std=0.02)

    def forward(self, tokens: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """Forward pass.

        Args:
            tokens: (B, L) int64 token ids.

        Returns:
            logits: (B, L, V)
            h: (B, L, d_model) hidden states
        """

        if tokens.ndim != 2:
            raise ValueError("tokens must be (B,L)")
        B, L = tokens.shape
        if L > self.cfg.max_seq_len:
            raise ValueError("sequence too long for configured max_seq_len")

        device = tokens.device
        pos = torch.arange(L, device=device).unsqueeze(0).expand(B, L)
        x = self.tok_emb(tokens) + self.pos_emb(pos)

        mask = _causal_mask(L, device=device)
        h = self.transformer(x, mask=mask)
        logits = self.lm_head(h)
        return logits, h

    @torch.no_grad()
    def embed(self, tokens: torch.Tensor, pool: str = "last") -> torch.Tensor:
        """Extract a stable embedding from a token sequence.

        Args:
            tokens: (B, L) int64
            pool: "last" or "mean"

        Returns:
            z: (B, d_model)
        """

        _, h = self(tokens)
        if pool == "last":
            return h[:, -1, :]
        if pool == "mean":
            return h.mean(dim=1)
        raise ValueError("pool must be 'last' or 'mean'")


@torch.no_grad()
def freeze_module(m: nn.Module) -> None:
    """Freeze a module in-place."""

    m.eval()
    for p in m.parameters():
        p.requires_grad = False
